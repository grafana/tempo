import logging
import itertools
from dataclasses import dataclass, field
from time import perf_counter
from typing import Optional, Generator, List, Callable, Union, Iterable

import pandas as pd
from joblib import Parallel, delayed

from mlt.models.base_models import MLTBaseModel
from mlt.typing import CVSplitter


@dataclass
class PredictionResults:
    """
    Container class for storing prediction results from a training pipeline run.

    Attributes:
        out_of_sample_predictions (pd.DataFrame, optional): Out-of-sample predictions generated by the model
        in_sample_predictions (pd.DataFrame, optional): In-sample predictions on training data
        shaps (pd.DataFrame, optional): SHAP values explaining model predictions
        model_params (pd.DataFrame, optional): Model hyperparameters used for training
        feature_importances (pd.DataFrame, optional): Feature importance scores
        prediction_end_time (str, optional): End timestamp of the prediction period
        model (MLTBaseModel, optional): Trained model instance
        inner_cv_params (pd.DataFrame): Cross-validation parameters from inner CV loop
        inner_cv_results (pd.DataFrame): Cross-validation results from inner CV loop
    """

    out_of_sample_predictions: Optional[pd.DataFrame] = None
    in_sample_predictions: Optional[pd.DataFrame] = None
    shaps: Optional[pd.DataFrame] = None
    model_params: Optional[pd.DataFrame] = None
    feature_importances: Optional[pd.DataFrame] = None
    prediction_end_time: Optional[str] = None
    # It might be better to pass the model around another way but I'm not sure what's better
    model: MLTBaseModel = None
    inner_cv_params: pd.DataFrame = None
    inner_cv_results: pd.DataFrame = None

    @classmethod
    def concat(cls, results: Iterable["PredictionResults"]) -> "PredictionResults":
        """
        Concatenate multiple PredictionResults objects into a single combined result.

        Args:
            results (Iterable[PredictionResults]): An iterable of PredictionResults objects to combine

        Returns:
            PredictionResults: A new PredictionResults object containing the concatenated data from all input results,
                with fields combined appropriately
        """
        results = sorted(results, key=lambda r: r.prediction_end_time)

        return PredictionResults(
            out_of_sample_predictions=cls._concat_df_field(results, lambda r: r.out_of_sample_predictions),
            in_sample_predictions=cls._concat_df_field(results, lambda r: r.in_sample_predictions).reset_index(
                drop=True
            ),
            shaps=cls._concat_df_field(results, lambda r: r.shaps),
            model_params=cls._concat_df_field(results, lambda r: r.model_params),
            feature_importances=cls._concat_df_field(results, lambda r: r.feature_importances),
            inner_cv_results=cls._concat_df_field(results, lambda r: r.inner_cv_results),
            inner_cv_params=cls._concat_df_field(results, lambda r: r.inner_cv_params),
        )

    @staticmethod
    def _concat_df_field(
        results: Iterable["PredictionResults"], field_getter: Callable[["PredictionResults"], pd.DataFrame]
    ) -> Union[None, pd.DataFrame]:
        """
        Helper method to concatenate DataFrame fields from multiple PredictionResults objects.

        Args:
            results (Iterable[PredictionResults]): An iterable of PredictionResults objects
            field_getter (Callable[[PredictionResults], pd.DataFrame]): Function to extract the DataFrame field from each result

        Returns:
            Union[None, pd.DataFrame]: Concatenated DataFrame if any non-None fields exist, otherwise None
        """
        df_list = [df for r in results if (df := field_getter(r)) is not None]
        if not df_list:
            return None

        return pd.concat([df for r in results if (df := field_getter(r)) is not None])


@dataclass
class PredictionTask:
    """
    A dataclass representing a single prediction task with training and prediction data.

    Attributes:
        training_data (pd.DataFrame): Data used for training the model
        prediction_data (pd.DataFrame): Data to generate predictions on
        is_tune_task (bool): Whether this task is for tuning hyperparameters
        is_smoke_test (bool): Whether this is a smoke test run
        is_last_task (bool): Whether this is the final task in the sequence
        min_shap_date (str): Earliest date to start calculating SHAP values
        prediction_time_column_name (str): Name of column containing prediction timestamps
        min_train_time (pd.Timestamp): Minimum timestamp in training data (set in post_init)
        max_train_time (pd.Timestamp): Maximum timestamp in training data (set in post_init)
    """

    training_data: pd.DataFrame
    prediction_data: pd.DataFrame
    is_tune_task: bool
    is_smoke_test: bool
    is_last_task: bool
    min_shap_date: str
    prediction_time_column_name: str
    min_train_time: pd.Timestamp = field(init=False)
    max_train_time: pd.Timestamp = field(init=False)

    def __post_init__(self):
        self.min_train_time = self.training_data[self.prediction_time_column_name].min()
        self.max_train_time = self.training_data[self.prediction_time_column_name].max()
        if self.is_smoke_test:
            # force the shap data to be saved in the case of a smoke test
            self.min_shap_date = self.min_train_time.strftime("%Y-%m-%d")
            self.is_last_task = True


class TrainingPipeline:
    """
    A pipeline for training and evaluating machine learning models across multiple time periods.

    This class manages the process of:
    - Splitting data into training and prediction sets using a CVSplitter
    - Training models with periodic hyperparameter tuning
    - Generating and storing predictions
    - Calculating model metrics and artifacts (feature importance, SHAP values)

    The pipeline supports:
    - Configurable tuning frequency
    - Smoke test mode for quick validation
    - Parallel processing of outer CV folds
    - Selective SHAP value calculation based on date ranges
    - Storage of both cumulative and final results

    Attributes:
        prediction_model (MLTBaseModel): Model object wrapping classification/regression model
        prediction_date_manager (CVSplitter): Manager for prediction training/prediction windows
        tuning_frequency (int): Number of iterations between hyperparameter tuning
        min_shap_date (str): First date to start calculating SHAP values
        prediction_time_column_name (str): Column name containing prediction timestamps
        num_outer_cv_jobs (int): Number of parallel jobs for outer CV
        is_smoke_test (bool): Flag for running abbreviated smoke test
        all_results_ (PredictionResults): Cumulative results across all iterations
        last_results_ (PredictionResults): Results from final iteration only
    """

    def __init__(
        self,
        prediction_model: MLTBaseModel,
        prediction_date_manager: CVSplitter,
        tuning_frequency: int,
        min_shap_date: str,
        prediction_time_column_name: str,
        num_outer_cv_jobs: int = 1,
        is_smoke_test: bool = False,
    ):
        """
        Initialize the TrainingPipeline.

        Args:
            prediction_model (MLTBaseModel): Model object wrapping classification/regression model
            prediction_date_manager (CVSplitter): Manager for prediction training/prediction windows
            tuning_frequency (int): Number of iterations between hyperparameter tuning
            min_shap_date (str): First date to start calculating SHAP values
            prediction_time_column_name (str): Column name containing prediction timestamps 
            num_outer_cv_jobs (int, optional): Number of parallel jobs for outer CV. Defaults to 1.
            is_smoke_test (bool, optional): Flag for running abbreviated smoke test. Defaults to False.
        """
        self.logger = logging.getLogger(__name__)
        self.tuning_frequency = tuning_frequency
        self.prediction_model = prediction_model
        self.prediction_date_manager = prediction_date_manager
        self.model_type = prediction_model.model_type
        self.min_shap_date = min_shap_date
        self.num_outer_cv_jobs = num_outer_cv_jobs
        self.is_smoke_test = is_smoke_test
        self.prediction_time_column_name = prediction_time_column_name
        if self.is_smoke_test:
            self.logger.warning(5 * "\n" + "THIS IS A SMOKE TEST. ONLY ONE TUNE ITERATION WILL BE RUN" + 5 * "\n")
        self.all_results_: PredictionResults = PredictionResults()
        self.last_results_: PredictionResults = PredictionResults()

    def split_data(self, data: pd.DataFrame) -> Generator[PredictionTask, None, None]:
        """
        Split input data into training and prediction sets using the prediction date manager.

        Yields a series of PredictionTask objects containing the split data and metadata about
        whether this split requires tuning, is a smoke test, etc.

        Args:
            data (pd.DataFrame): Input DataFrame containing features and labels

        Yields:
            PredictionTask: Contains training data, prediction data, and task metadata
            for each split. The task metadata includes:
                - Whether hyperparameter tuning is needed
                - If this is a smoke test run
                - If this is the final prediction task
                - Minimum date for SHAP value calculation
                - Column name containing prediction timestamps

        Returns:
            Generator yielding PredictionTask objects
        """
        train_epochs_since_last_tuning = 0
        for train_indices, prediction_indices in self.prediction_date_manager.split(data):
            yield PredictionTask(
                training_data=self.prediction_model.get_fold_from_index(data, train_indices),
                prediction_data=self.prediction_model.get_fold_from_index(data, prediction_indices),
                is_tune_task=train_epochs_since_last_tuning % self.tuning_frequency == 0,
                is_smoke_test=self.is_smoke_test,
                is_last_task=prediction_indices[-1] == data.index[-1],
                min_shap_date=self.min_shap_date,
                prediction_time_column_name=self.prediction_time_column_name,
            )
            if self.is_smoke_test:
                return
            train_epochs_since_last_tuning += 1

    def get_tune_task_blocks(self, data: pd.DataFrame) -> Generator[List[PredictionTask], None, None]:
        """
        Split prediction tasks into blocks based on tuning requirements.

        This method groups prediction tasks into blocks, where each block starts with
        a tuning task (if required) followed by prediction tasks that use those tuned
        parameters. This allows for efficient parameter tuning while maintaining model
        stability between tuning points.

        Args:
            data (pd.DataFrame): Input DataFrame containing features and labels

        Yields:
            List[PredictionTask]: A block of prediction tasks, where the first task
                in each block is a tuning task if tuning is required. Each subsequent
                task in the block uses the tuned parameters from the first task.

        Notes:
            - Blocks are determined by the tuning_frequency parameter
            - Each block starts with a task where is_tune_task=True (except potentially
              the first block if starting mid-cycle)
            - For smoke tests, only a single block will be yielded
        """
        splitter = self.split_data(data)

        block = [next(splitter)]
        for task in splitter:
            if task.is_tune_task:
                yield block
                block = [task]
            else:
                block.append(task)
        yield block

    @staticmethod
    def run_prediction_task(model: MLTBaseModel, task: PredictionTask) -> PredictionResults:
        """
        Execute a single prediction task, ensuring that ARIMA is trained before making predictions.
        """
        results = PredictionResults()
        train_times_dict = dict(tune_start_time=task.min_train_time, tune_end_time=task.max_train_time)

        # Only tune hyperparameters if the model supports it
        if task.is_tune_task and hasattr(model, "tune_hyperparameters"):
            model.tune_hyperparameters(task.training_data)
            results.model_params = pd.DataFrame([model.model_parameters | train_times_dict])

            # Handle CV results properly (check if it's a list or dict)
            if isinstance(model.cv_results_, dict) and "results" in model.cv_results_:
                cv_results = model.cv_results_["results"]
            else:
                cv_results = model.cv_results_ if isinstance(model.cv_results_, list) else []

            results.inner_cv_results = pd.DataFrame(cv_results)
            if not results.inner_cv_results.empty:
                results.inner_cv_results["tune_start_time"] = train_times_dict["tune_start_time"]
                results.inner_cv_results["tune_end_time"] = train_times_dict["tune_end_time"]

            cv_params = model.cv_params_ if model.cv_params_ else {}  # Ensure it's a dictionary
            results.inner_cv_params = pd.DataFrame([{**cv_params, **train_times_dict}])

        # Ensure ARIMA is trained before making predictions
        if getattr(model, "best_model_", None) is None:
            model.train_model(task.training_data)

        # Generate predictions
        predictions = model.get_predictions(task.prediction_data.copy())
        prediction_end_time = predictions[task.prediction_time_column_name].max()
        predictions["in_sample_start_time"] = task.min_train_time
        predictions["in_sample_end_time"] = task.max_train_time
        predictions["is_tune_task"] = task.is_tune_task
        results.out_of_sample_predictions = predictions
        results.prediction_end_time = prediction_end_time

        # Handle feature importance for models that support it
        if model.has_feature_importance:
            feature_importance = model.feature_importances(task.training_data)
            feature_importance[f"{'tune' if task.is_tune_task else 'train'}_start_time"] = task.min_train_time
            feature_importance[f"{'tune' if task.is_tune_task else 'train'}_end_time"] = task.max_train_time
            results.feature_importances = feature_importance.sort_values("score", ascending=False)

        # Get in-sample predictions for analysis
        in_sample_predictions = model.get_predictions(task.training_data)
        added_cols = in_sample_predictions.columns.difference(task.training_data.columns)
        in_sample_predictions = in_sample_predictions[
            [x for x in in_sample_predictions.columns if x in added_cols.union(model.non_feature_columns)]
        ]
        in_sample_predictions["in_sample_start_time"] = task.min_train_time
        in_sample_predictions["in_sample_end_time"] = task.max_train_time
        results.in_sample_predictions = in_sample_predictions

        # Calculate SHAP values if the model supports them
        if model.has_shap and prediction_end_time.strftime("%Y-%m-%d") > task.min_shap_date:
            shaps = model.get_shap_values(feature_data=task.prediction_data, predictions=predictions)
            results.shaps = shaps

        # Store the model in results if it's the last task
        if task.is_last_task:
            results.model = model

        return results


    def generate_predictions_for_prediction_period(self, data_for_training: pd.DataFrame) -> PredictionResults:
        """
        Generate predictions for a given prediction period using the configured model.

        This method orchestrates the entire prediction pipeline:
        1. Creates prediction tasks based on the configured cross-validation strategy
        2. Executes tasks in parallel if num_outer_cv_jobs > 1
        3. Concatenates and sorts results from all tasks
        4. Stores the final model and results

        Parameters
        ----------
        data_for_training : pd.DataFrame
            DataFrame containing features and labels to use for model training

        Returns
        -------
        PredictionResults
            Object containing concatenated results from all prediction tasks including:
            - out_of_sample_predictions: Out-of-sample predictions for each CV fold
            - in_sample_predictions: In-sample predictions on training data
            - feature_importances: Feature importance scores if model supports them
            - shaps: SHAP values for predictions after min_shap_date if supported
            - model_params: Model hyperparameters used for predictions
            - inner_cv_results: Cross-validation results from hyperparameter tuning
        """
        tik = perf_counter()

        def run_task_block(block: List[PredictionTask], model: MLTBaseModel):
            return [self.run_prediction_task(model, task) for task in block]

        if self.num_outer_cv_jobs == 1:
            block_list = [
                run_task_block(b, self.prediction_model) for b in self.get_tune_task_blocks(data_for_training)
            ]
        else:
            block_list = Parallel(n_jobs=self.num_outer_cv_jobs)(
                delayed(run_task_block)(b, self.prediction_model) for b in self.get_tune_task_blocks(data_for_training)
            )

        results_list = itertools.chain.from_iterable(block_list)
        results_list = sorted(results_list, key=lambda r: r.prediction_end_time)
        all_results = PredictionResults.concat(results_list)
        self.all_results_ = all_results
        self.logger.info(f"Predictions Generated in {perf_counter() - tik}")

        last_results = results_list[-1]
        self.prediction_model = last_results.model
        self.last_results_ = last_results
        return all_results
